{
 "cells": [
  {
   "cell_type": "code",
   "id": "6e733cde-923c-4352-9291-81c1dedbc65b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T15:29:03.032350Z",
     "start_time": "2025-12-03T15:29:03.030223Z"
    }
   },
   "source": [
    "# headers = {\n",
    "# \t\t\"Content-Type\": \"application/json\",\n",
    "# \t\t\"Connection\": \"keep-alive\",\n",
    "# \t\t\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64; rv:120.0) Gecko/20100101 Firefox/120.0\",\n",
    "# \t\t\"Access-Control-Allow-Origin\": \"*\"\n",
    "# }\n",
    "\n",
    "# import re\n",
    "\n",
    "# def remove_whitespace(s: str, keep_spaces: bool = True) -> str:\n",
    "#     \"\"\"\n",
    "#     Clean whitespace while keeping natural spaces between words.\n",
    "\n",
    "#     - Convert all non-space whitespace (tabs, newlines, etc.) into a space\n",
    "#     - Collapse multiple spaces into one\n",
    "#     - Trim leading/trailing spaces\n",
    "#     \"\"\"\n",
    "#     # Replace tabs/newlines/etc. with a space\n",
    "#     s = re.sub(r\"[^\\S ]+\", \" \", s)\n",
    "\n",
    "#     # Collapse multiple spaces into one\n",
    "#     s = re.sub(r\" +\", \" \", s)\n",
    "    \n",
    "#     # Replace \\ by \"\"\n",
    "#     s = re.sub(r\"\\\\\", \"\", s)\n",
    "    \n",
    "#     # Trim\n",
    "#     return s.strip()\n",
    "\n",
    "\n",
    "# def get_specific_element(regex: str, text: str):\n",
    "#     m = re.search(regex, text)\n",
    "#     return m.group(0) if m else None\n",
    "\n",
    "\n",
    "# def safe_text(element, keep_spaces=False):\n",
    "#     if not element:\n",
    "#         return \"\"\n",
    "#     try:\n",
    "#         e = element.text\n",
    "#     except AttributeError:\n",
    "#         e = element\n",
    "#     return remove_whitespace(e, keep_spaces)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "bca174b9-0844-47a1-b65b-57760606e685",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T15:29:03.048526Z",
     "start_time": "2025-12-03T15:29:03.046311Z"
    }
   },
   "source": [
    "# from selenium.webdriver.common.keys import Keys\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.common.action_chains import ActionChains\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.support.ui import Select\n",
    "# from bs4 import BeautifulSoup\n",
    "# from time import sleep\n",
    "\n",
    "\n",
    "# def scrape_annonce(url, htmlContent):\n",
    "#     \"\"\"\n",
    "#     Scrapes a ParuVendu annonce page:\n",
    "#     - Opens Selenium\n",
    "#     - Accepts cookies\n",
    "#     - Clicks `Lire plus` when present\n",
    "#     - Extracts description using Selenium\n",
    "#     - Parses remaining fields using BeautifulSoup\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # ---------- SELENIUM ----------\n",
    "#     opt = Options()\n",
    "#     opt.add_argument(\"start-maximized\")\n",
    "#     opt.add_argument(\"--lang=en-US\")\n",
    "\n",
    "#     driver = webdriver.Chrome(\n",
    "#         service=Service(ChromeDriverManager().install()),\n",
    "#         options=opt\n",
    "#     )\n",
    "\n",
    "#     driver.get(url)\n",
    "#     sleep(2)\n",
    "\n",
    "#     try:\n",
    "#         btn = driver.find_element(By.XPATH, \"//button[text()='Accepter']\")\n",
    "#         driver.execute_script(\"arguments[0].click();\", btn)\n",
    "#         sleep(1)\n",
    "#     except:\n",
    "#         pass  \n",
    "\n",
    "#     # Click \"Lire plus\"\n",
    "#     try:\n",
    "#         read_more = driver.find_element(By.ID, \"linkAnnonceTrunc\")\n",
    "#         driver.execute_script(\"arguments[0].click();\", read_more)\n",
    "#         sleep(2)\n",
    "#     except Exception as e:\n",
    "#         print(f\"[⚠] Lire plus not found: {e}\")\n",
    "\n",
    "#     # Extract description\n",
    "#     try:\n",
    "#         element = driver.find_element(\n",
    "#             By.XPATH, \"//div[@class='im12_txt_ann im12_txt_ann_auto']\"\n",
    "#         )\n",
    "#         description = element.text\n",
    "#     except:\n",
    "#         description = \"\"\n",
    "\n",
    "#     driver.quit()\n",
    "\n",
    "#     # ---------- BEAUTIFULSOUP PART ----------\n",
    "#     data = {}\n",
    "\n",
    "#     if htmlContent:\n",
    "#         soup = BeautifulSoup(htmlContent, 'html.parser')\n",
    "\n",
    "#         data[\"title\"] = safe_text(\n",
    "#             soup.find(\"span\", {\"id\": \"detail_h1\"}), True\n",
    "#         )\n",
    "\n",
    "#         data[\"location\"] = safe_text(\n",
    "#             soup.find(\"span\", {\"id\": \"detail_loc\"})\n",
    "#         )\n",
    "\n",
    "#         data[\"descriptionTitle\"] = safe_text(\n",
    "#             soup.find(\"h2\", {\"class\": \"autodetail-titre sepdetail14-ssbordure\"})\n",
    "#         )\n",
    "\n",
    "#         data[\"description\"] = safe_text(description)\n",
    "\n",
    "#         data[\"nbp\"] = get_specific_element(\n",
    "#             r\"\\d+\",\n",
    "#             safe_text(soup.find(\"li\", {\"class\": \"nbp\"}))\n",
    "#         )\n",
    "\n",
    "#         data[\"surf\"] = get_specific_element(\n",
    "#             r\"\\d+\",\n",
    "#             safe_text(soup.find(\"li\", {\"class\": \"surf\"}))\n",
    "#         )\n",
    "\n",
    "#         data[\"prix\"] = get_specific_element(\n",
    "#             r\"\\d+(?: \\d+)*\",\n",
    "#             safe_text(soup.find(\"div\", {\"class\": \"prixactionalerte-box\"}))\n",
    "#         )\n",
    "\n",
    "#         data[\"enseigneInfosvendeur\"] = safe_text(\n",
    "#             soup.find(\"p\", {\"class\": \"enseigne-infosvendeur\"})\n",
    "#         )\n",
    "\n",
    "#     return data\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "0a9b0720-61f6-4a8a-ac68-d0c77e25fe86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T15:29:03.057392Z",
     "start_time": "2025-12-03T15:29:03.055724Z"
    }
   },
   "source": [
    "# import requests\n",
    "# import os \n",
    "\n",
    "\n",
    "# annonces_link_csv = \"annonces_link.csv\"\n",
    "\n",
    "# if os.path.exists(annonces_link_csv):\n",
    "#     with open(annonces_link_csv, \"r\") as f:\n",
    "#         links = f.readlines()\n",
    "#         for link in links :  \n",
    "#             annonceUrl = remove_whitespace(link)\n",
    "\n",
    "#             print(\"On charge le site : \" + link)\n",
    "#             response = requests.get(annonceUrl,headers=headers)\n",
    "            \n",
    "#             content = \"\"\n",
    "            \n",
    "    \n",
    "#             if response.status_code == 200:\n",
    "#                 print(\"Site web chargé\")\n",
    "#                 content = response.text\n",
    "#             else:\n",
    "#                 print(\"Erreur lors du chargement de la page , Status Code : \"+response.status_code)\n",
    "        \n",
    "                \n",
    "#             result = scrape_annonce(\n",
    "#                 url=annonceUrl,\n",
    "#                 htmlContent=content)\n",
    "\n",
    "#             print(result)\n",
    "\n",
    "        \n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "1a24d32b-f199-46b7-b33c-4b3d44ff957a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T15:29:03.272445Z",
     "start_time": "2025-12-03T15:29:03.073309Z"
    }
   },
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import requests\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from datetime import datetime\n",
    "\n",
    "# -------------------- CONFIG --------------------\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64; rv:120.0) Gecko/20100101 Firefox/120.0\",\n",
    "    \"Access-Control-Allow-Origin\": \"*\"\n",
    "}\n",
    "\n",
    "input_csv = \"annonces_link.csv\"    \n",
    "output_csv = \"results.csv\"          \n",
    "\n",
    "# -------------------- UTILITIES --------------------\n",
    "def remove_whitespace(s: str, keep_spaces: bool = True) -> str:\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = re.sub(r\"[^\\S ]+\", \" \", s)\n",
    "    s = re.sub(r\" +\", \" \", s)\n",
    "    s = re.sub(r\"\\\\\", \"\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def get_specific_element(regex: str, text: str):\n",
    "    if not text:\n",
    "        return None\n",
    "    m = re.search(regex, text)\n",
    "    return m.group(0) if m else None\n",
    "\n",
    "def safe_text(element, keep_spaces=False):\n",
    "    if not element:\n",
    "        return \"\"\n",
    "    try:\n",
    "        e = element.text\n",
    "    except AttributeError:\n",
    "        e = element\n",
    "    return remove_whitespace(e, keep_spaces)\n",
    "\n",
    "def stringToTimeStamp(date_str:str):\n",
    "    dt = datetime.strptime(date_str, \"%d/%m/%Y %H:%M\")\n",
    "    return int(dt.timestamp())\n",
    "    \n",
    "\n",
    "def safe_find_text(soup, tag, attrs=None, keep_spaces=False):\n",
    "    element = soup.find(tag, attrs=attrs)\n",
    "    return safe_text(element, keep_spaces) if element else \"\"\n",
    "\n",
    "def scrape_annonce(url, htmlContent):\n",
    "    \"\"\"\n",
    "    Scrapes a ParuVendu annonce page:\n",
    "    - Selenium: handles dynamic content & clicks 'Lire plus'\n",
    "    - BeautifulSoup: parses static content\n",
    "    \"\"\"\n",
    "    # ---------- SELENIUM ----------\n",
    "    opt = Options()\n",
    "    opt.add_argument(\"start-maximized\")\n",
    "    opt.add_argument(\"--lang=en-US\")\n",
    "\n",
    "    driver = webdriver.Chrome(\n",
    "        service=Service(ChromeDriverManager().install()),\n",
    "        options=opt\n",
    "    )\n",
    "\n",
    "    driver.get(url)\n",
    "    sleep(2)\n",
    "\n",
    "    # Accept cookies\n",
    "    try:\n",
    "        btn = driver.find_element(By.XPATH, \"//button[text()='Accepter']\")\n",
    "        driver.execute_script(\"arguments[0].click();\", btn)\n",
    "        sleep(1)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Cliquer sur  \"Lire plus\" si ca exite \n",
    "    try:\n",
    "        read_more = driver.find_element(By.ID, \"linkAnnonceTrunc\")\n",
    "        driver.execute_script(\"arguments[0].click();\", read_more)\n",
    "        sleep(1)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Extraire la description\n",
    "    try:\n",
    "        element = driver.find_element(By.XPATH, \"//div[@class='im12_txt_ann im12_txt_ann_auto']\")\n",
    "        description = element.text\n",
    "    except:\n",
    "        description = \"\"\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # ---------- BEAUTIFULSOUP ----------\n",
    "    data = {}\n",
    "    if htmlContent:\n",
    "        soup = BeautifulSoup(htmlContent, 'html.parser')\n",
    "        data[\"title\"] = safe_find_text(soup, \"span\", {\"id\": \"detail_h1\"}, True)\n",
    "        data[\"location\"] = safe_find_text(soup, \"span\", {\"id\": \"detail_loc\"})\n",
    "        data[\"descriptionTitle\"] = safe_find_text(soup, \"h2\", {\"class\": \"autodetail-titre sepdetail14-ssbordure\"})\n",
    "        data[\"description\"] = safe_text(description)\n",
    "        data[\"nbp\"] = get_specific_element(r\"\\d+\", safe_find_text(soup, \"li\", {\"class\": \"nbp\"}))\n",
    "        data[\"surf\"] = get_specific_element(r\"\\d+\", safe_find_text(soup, \"li\", {\"class\": \"surf\"}))\n",
    "        data[\"prix\"] = get_specific_element(r\"\\d+(?: \\d+)*\", safe_find_text(soup, \"div\", {\"class\": \"prixactionalerte-box\"}))\n",
    "        data[\"enseigneInfosvendeur\"] = safe_find_text(soup, \"p\", {\"class\": \"enseigne-infosvendeur\"})\n",
    "        # data[\"enseigneInfosvendeur\"] = safe_find_text(soup, \"p\", {\"class\": \"enseigne-infosvendeur\"})\n",
    "\n",
    "        uls = soup.find_all(\"ul\", {\"class\": \"crit-alignbloc\"})\n",
    "        \n",
    "        for idx, ul in enumerate(uls, 1):\n",
    "            datas = {}\n",
    "            li_items = ul.find_all(\"li\")\n",
    "        \n",
    "            for li in li_items:\n",
    "        \n",
    "                strong = li.find(\"strong\")\n",
    "                if strong:  \n",
    "                    # category is inside <strong>\n",
    "                    category = strong.get_text(strip=True)\n",
    "                else:\n",
    "                    # category is plain text before spans\n",
    "                    category = li.get_text(\" \", strip=True).split(\" \")[0]\n",
    "        \n",
    "                values = [span.get_text(strip=True) for span in li.find_all(\"span\")]\n",
    "                datas[category] = values\n",
    "        print(datas)\n",
    "        \n",
    "        data[\"agencement\"] = \",\".join(datas.get(\"Agencement\", []))\n",
    "        data[\"general\"] = \",\".join(datas.get(\"Général\", []))\n",
    "        data[\"annexe\"] = \",\".join(datas.get(\"Annexes\", []))\n",
    "\n",
    "        data[\"dependance\"] = \",\".join(datas.get(\"Dépendance\", []))\n",
    "\n",
    "        # Tranformer la date en timestamp\n",
    "        raw_date = datas.get(\"Mise\", [])\n",
    "        if raw_date:\n",
    "            date_str = raw_date[0].replace(\" à \", \" \")\n",
    "            data[\"publishedAt\"] = stringToTimeStamp(date_str)\n",
    "        else:\n",
    "            data[\"publishedAt\"] = None\n",
    "\n",
    "\n",
    "        data[\"reference\"] =  safe_text(datas.get(\"Réf.\")[0],[])\n",
    "\n",
    "    return data\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nebel/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "69156a0a-eda0-4209-90ea-ad127acaf871",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T15:29:15.464279Z",
     "start_time": "2025-12-03T15:29:03.277323Z"
    }
   },
   "source": [
    "link= \"https://www.paruvendu.fr/immobilier/vente/maison/1283596970A1KIVHMN000\"\n",
    "response = requests.get(link, headers=headers)\n",
    "if response.status_code == 200:\n",
    "    content = response.text\n",
    "    data = scrape_annonce(link, content)\n",
    "    print(data)\n",
    "else :\n",
    "    print(f\"Error loading page: {response.status_code}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Agencement': ['4 chambres', 'Cheminée'], 'Général': ['Chauffage : électrique radiateur'], 'Extérieur': ['Surface  terrain : 242\\r\\nm2'], 'Dépendance': ['Terrasse'], 'Réf.': ['ParuVendu \\t\\t\\t\\t\\t\\t1789197'], 'Mise': ['18/11/2025 à 06:36']}\n",
      "{'title': 'Vente Maison 8 pièces 158 m²', 'location': 'Pierrelaye (95480)', 'descriptionTitle': 'Description T8 à Pierrelaye', 'description': \"Vente Maison/villa 8 pièces iad France - Fabien Pruvot vous propose: Maison de caractère à Pierrelaye, à 7 min à pied de la gare Située dans un quartier pavillonnaire très recherché, cette bâtisse aux volumes généreux ne manquera pas de vous séduire ! Édifiée sur trois niveaux de 70m² au sol et complétée par une cave voûtée de 22m², elle offre un véritable potentiel pour les amateurs d'espace. Au rez-de-chaussée surélevé, vous serez accueilli par une vaste pièce de vie lumineuse, dotée d'une cuisine semi-ouverte meublée et équipée, une chambre, une salle d'eau avec WC. À l'étage, un grand palier dessert deux chambres, un bureau (anciennement salle de bain, facile à réaménager) et un WC indépendant. En rez-de-jardin, profitez d'un second séjour, d'un grand dressing, d'une buanderie et d'un troisième WC. Un espace idéal où vos envies prennent vie. En sous-sol, la cave voûtée de 22m² est parfaite pour le stockage ou pour les amateurs de vin. Le tout exposé plein sud, pour un ensoleillement optimal toute la journée. Une maison coup de cOEur, pleine de charme et de potentiel, parfaite pour les familles ou les passionnés de beaux volumes à réinventer. À découvrir sans tarder ! Honoraires d'agence à la charge du vendeur. Information d'affichage énergétique sur ce bien : classe ENERGIE F indice 375 et classe CLIMAT C indice 12. Les informations sur les risques auxquels ce bien est exposé, y compris l'obligation légale de débroussaillement, sont disponibles sur le site Géorisques : La présente annonce immobilière a été rédigée sous la responsabilité éditoriale de M Fabien Pruvot mandataire indépendant en immobilier (sans détention de fonds), agent commercial de la SAS I@D France immatriculé au RSAC de Pontoise sous le numéro 895227825, titulaire de la carte de démarchage immobilier pour le compte de la société I@D France SAS. CC HT Numéro de mandat : 1789197 Honoraires à la charge du Vendeur Bien En copropriété : NON\", 'nbp': '8', 'surf': '158', 'prix': '375 000', 'enseigneInfosvendeur': 'iad France Agent Commercial immatriculé à CCI SEINE ET MARNE N° 50367642100020 Fabien Pruvot', 'agencement': '4 chambres,Cheminée', 'general': 'Chauffage : électrique radiateur', 'annexe': '', 'dependance': 'Terrasse', 'publishedAt': 1763444160, 'reference': 'ParuVendu 1789197'}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "fc1bffeb-5234-4999-89ad-6763242a1db2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T15:29:32.432098Z",
     "start_time": "2025-12-03T15:29:15.512537Z"
    }
   },
   "source": [
    "processed_links = set()\n",
    "if os.path.exists(output_csv):\n",
    "    with open(output_csv, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            if row:\n",
    "                processed_links.add(row[0].strip())\n",
    "\n",
    "# Lire tout les links a scraper \n",
    "\n",
    "if os.path.exists(input_csv):\n",
    "    with open(input_csv, \"r\", encoding=\"utf-8\") as f:\n",
    "        all_links = [line.strip() for line in f if line.strip()]\n",
    "else:\n",
    "    print(\"Input CSV not found.\")\n",
    "    all_links = []\n",
    "\n",
    "# Ouvrir  results CSV en monde append (pour rajouter des lignes)\n",
    "with open(output_csv, \"a\", encoding=\"utf-8\", newline=\"\") as out_file:\n",
    "    writer = csv.writer(out_file)\n",
    "\n",
    "    for idx,link in enumerate(all_links):\n",
    "        if link in processed_links:\n",
    "            continue  \n",
    "\n",
    "        try:\n",
    "            print(f\"Processing: {link}  ({idx}/{len(all_links)})\")\n",
    "            response = requests.get(link, headers=headers)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Error loading page: {response.status_code}\")\n",
    "                continue\n",
    "\n",
    "            content = response.text\n",
    "            data = scrape_annonce(link, content)\n",
    "\n",
    "          \n",
    "            writer.writerow([link, data])\n",
    "            out_file.flush()\n",
    "\n",
    "            sleep(2)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {link}: {e}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: https://www.paruvendu.fr/immobilier/vente/appartement/1282974090A1KIVHAP000  (0/30030)\n",
      "Error loading page: 410\n",
      "Processing: https://www.paruvendu.fr/immobilier/vente/appartement/1265798356A1KIVHAP000  (1/30030)\n",
      "{'Agencement': [\"1 salle d'eau\"], 'Général': ['Etage : 4 (dernier étage)', \"Nombre d'étage(s) dans l'immeuble : 4\"], 'Accès': ['Digicode', 'Interphone', 'Concierge'], 'Annexes': ['Parking'], 'Réf.': ['ParuVendu \\t\\t\\t\\t\\t\\t13183462_979'], 'Mise': ['15/11/2025 à 05:15']}\n",
      "Processing: https://www.paruvendu.fr/immobilier/vente/appartement/1275249674A1KIVHAP000  (2/30030)\n",
      "{'Agencement': ['1 chambre', \"1 salle d'eau\"], 'Général': ['Etage : 6 (dernier étage)', \"Nombre d'étage(s) dans l'immeuble : 6\"], 'Accès': ['Ascenseur', 'Digicode', 'Interphone', 'Concierge'], 'Dépendance': ['Balcon'], 'Annexes': ['Parking'], 'Réf.': ['ParuVendu \\t\\t\\t\\t\\t\\t13600420_1201'], 'Mise': ['26/11/2025 à 05:15']}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 40\u001B[0m\n\u001B[1;32m     37\u001B[0m     writer\u001B[38;5;241m.\u001B[39mwriterow([link, data])\n\u001B[1;32m     38\u001B[0m     out_file\u001B[38;5;241m.\u001B[39mflush()\n\u001B[0;32m---> 40\u001B[0m     \u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     43\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError processing \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlink\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3c1593-bf3b-4708-be71-191dae56b506",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ### **Agency**\n",
    "\n",
    "# * `id`\n",
    "# * `name`\n",
    "# * `address`\n",
    "# * `slogan`\n",
    "# * `description`\n",
    "# * `telephone`\n",
    "# * `announcements` (relationship)\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### **Announcement**\n",
    "\n",
    "# * `id`\n",
    "# * `ref`\n",
    "# * `title`\n",
    "# * `description`\n",
    "# * `price`\n",
    "# * `price_per_meter`\n",
    "# * `publish_at`\n",
    "# * `updated_at`\n",
    "# * `nb_rooms`\n",
    "# * `nb_bedrooms`\n",
    "# * `exclusive`\n",
    "# * `dpe`\n",
    "# * `location`\n",
    "# * `url`\n",
    "# * `type`\n",
    "# * `agency_id`\n",
    "# * `agency` (relationship)\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### **Caracteristic**\n",
    "\n",
    "# * `id`\n",
    "# * `announcement_id`\n",
    "# * `announcement` (relationship)\n",
    "# * `parking_garage`\n",
    "# * `garden`\n",
    "# * `balcony_terrace`\n",
    "# * `annexes`\n",
    "# * `access`\n",
    "# * `arrangement`\n",
    "# * `dependence`\n",
    "# * `outside`\n",
    "# * `connectivity_index`\n",
    "# * `fiber_eligibility_rate`\n",
    "# * `general_information`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
